{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E 0.32723887476022145\n",
      "E 0.32719681129987715\n",
      "E 0.3271541814170747\n",
      "E 0.3271109738068431\n",
      "E 0.32706717686559156\n",
      "E 0.32702277868129964\n",
      "E 0.3269777670233218\n",
      "E 0.326932129331791\n",
      "E 0.326885852706598\n",
      "E 0.32683892389593366\n",
      "E 0.326791329284366\n",
      "E 0.3267430548804374\n",
      "E 0.32669408630375457\n",
      "E 0.3266444087715491\n",
      "E 0.32659400708468384\n",
      "E 0.32654286561307677\n",
      "E 0.32649096828051594\n",
      "E 0.3264382985488355\n",
      "E 0.3263848394014199\n",
      "E 0.3263305733260059\n",
      "E 0.32627548229674547\n",
      "E 0.32621954775549383\n",
      "E 0.3261627505922828\n",
      "E 0.32610507112493925\n",
      "E 0.3260464890778048\n",
      "E 0.3259869835595098\n",
      "E 0.32592653303975466\n",
      "E 0.3258651153250453\n",
      "E 0.32580270753332874\n",
      "E 0.32573928606747105\n",
      "E 0.32567482658751595\n",
      "E 0.3256093039816597\n",
      "E 0.32554269233587185\n",
      "E 0.3254749649020897\n",
      "E 0.3254060940649098\n",
      "E 0.3253360513066906\n",
      "E 0.3252648071709816\n",
      "E 0.3251923312241857\n",
      "E 0.3251185920153528\n",
      "E 0.3250435570340014\n",
      "E 0.32496719266585816\n",
      "E 0.3248894641463897\n",
      "E 0.3248103355120056\n",
      "E 0.324729769548794\n",
      "E 0.3246477277386453\n",
      "E 0.32456417020261097\n",
      "E 0.32447905564133234\n",
      "E 0.3243923412723624\n",
      "E 0.32430398276419686\n",
      "E 0.32421393416680916\n",
      "E 0.3241221478384789\n",
      "E 0.3240285743686839\n",
      "E 0.3239331624968087\n",
      "E 0.32383585902641\n",
      "E 0.32373660873475596\n",
      "E 0.3236353542773381\n",
      "E 0.3235320360870337\n",
      "E 0.3234265922675727\n",
      "E 0.32331895848093467\n",
      "E 0.32320906782827924\n",
      "E 0.3230968507239793\n",
      "E 0.3229822347622965\n",
      "E 0.32286514457620186\n",
      "E 0.3227455016878096\n",
      "E 0.3226232243498463\n",
      "E 0.3224982273775388\n",
      "E 0.32237042197025184\n",
      "E 0.32223971552215447\n",
      "E 0.3221060114211409\n",
      "E 0.3219692088351634\n",
      "E 0.3218292024850723\n",
      "E 0.3216858824029832\n",
      "E 0.3215391336751103\n",
      "E 0.3213888361679195\n",
      "E 0.3212348642363579\n",
      "E 0.32107708641281363\n",
      "E 0.32091536507534585\n",
      "E 0.3207495560936035\n",
      "E 0.32057950845071065\n",
      "E 0.3204050638392531\n",
      "E 0.3202260562293389\n",
      "E 0.32004231140652356\n",
      "E 0.31985364647720144\n",
      "E 0.319659869338852\n",
      "E 0.31946077811228984\n",
      "E 0.31925616053282224\n",
      "E 0.31904579329692784\n",
      "E 0.3188294413607689\n",
      "E 0.31860685718650733\n",
      "E 0.3183777799320261\n",
      "E 0.3181419345792506\n",
      "E 0.3178990309958151\n",
      "E 0.31764876292432803\n",
      "E 0.3173908068929544\n",
      "E 0.3171248210404331\n",
      "E 0.3168504438480029\n",
      "E 0.3165672927699921\n",
      "E 0.3162749627540455\n",
      "E 0.3159730246411\n",
      "E 0.3156610234342825\n",
      "E 0.3153384764248695\n",
      "E 0.31500487116232573\n",
      "E 0.3146596632542104\n",
      "E 0.3143022739804037\n",
      "E 0.3139320877046563\n",
      "E 0.31354844906489615\n",
      "E 0.3131506599220391\n",
      "E 0.312737976045233\n",
      "E 0.31230960350954545\n",
      "E 0.3118646947800562\n",
      "E 0.31140234445419374\n",
      "E 0.3109215846319532\n",
      "E 0.31042137988140556\n",
      "E 0.30990062176471533\n",
      "E 0.30935812288778863\n",
      "E 0.30879261043480677\n",
      "E 0.30820271914739383\n",
      "E 0.30758698370723453\n",
      "E 0.30694383048087176\n",
      "E 0.30627156858651156\n",
      "E 0.3055683802454541\n",
      "E 0.3048323103858297\n",
      "E 0.30406125547449225\n",
      "E 0.3032529515652293\n",
      "E 0.3024049615692553\n",
      "E 0.30151466177899205\n",
      "E 0.30057922771063433\n",
      "E 0.2995956193777745\n",
      "E 0.2985605661709925\n",
      "E 0.29747055160128316\n",
      "E 0.2963217982740858\n",
      "E 0.2951102536023282\n",
      "E 0.2938315769496383\n",
      "E 0.2924811291285882\n",
      "E 0.2910539654750186\n",
      "E 0.28954483409100046\n",
      "E 0.2879481813095145\n",
      "E 0.28625816699670065\n",
      "E 0.2844686929832119\n",
      "E 0.2825734487091618\n",
      "E 0.2805659790697123\n",
      "E 0.2784397804320142\n",
      "E 0.2761884317971385\n",
      "E 0.2738057689904608\n",
      "E 0.2712861103958077\n",
      "E 0.2686245428196846\n",
      "E 0.26581727517592263\n",
      "E 0.2628620652724105\n",
      "E 0.25975872038325015\n",
      "E 0.25650966475115083\n",
      "E 0.25312055600309996\n",
      "E 0.2496009173228929\n",
      "E 0.2459647334860441\n",
      "E 0.24223093816438043\n",
      "E 0.23842370067826643\n",
      "E 0.2345724081151953\n",
      "E 0.2307112406519317\n",
      "E 0.22687826147814769\n",
      "E 0.2231139929078151\n",
      "E 0.2194595259907035\n",
      "E 0.21595430194688692\n",
      "E 0.21263379016224826\n",
      "E 0.2095273438184768\n",
      "E 0.20665651791645256\n",
      "E 0.20403407589907643\n",
      "E 0.2016638001669612\n",
      "E 0.19954108634309264\n",
      "E 0.19765417710760522\n",
      "E 0.19598580920163444\n",
      "E 0.1945150212124704\n",
      "E 0.19321889583266705\n",
      "E 0.19207407085374936\n",
      "E 0.19105792617001296\n",
      "E 0.19014942097077472\n",
      "E 0.1893296051381439\n",
      "E 0.18858185846366984\n",
      "E 0.18789192299395904\n",
      "E 0.1872477926075334\n",
      "E 0.1866395151638404\n",
      "E 0.18605895064383446\n",
      "E 0.18549951660564923\n",
      "E 0.18495594167833188\n",
      "E 0.18442403939335397\n",
      "E 0.18390050846342157\n",
      "E 0.18338276138696522\n",
      "E 0.18286878058440348\n",
      "E 0.18235699975265976\n",
      "E 0.18184620740450544\n",
      "E 0.1813354693598427\n",
      "E 0.1808240670706557\n",
      "E 0.18031144894597886\n",
      "E 0.1797971922025948\n",
      "E 0.17928097314193114\n",
      "E 0.17876254410952375\n",
      "E 0.1782417157129633\n",
      "E 0.17771834315056903\n",
      "E 0.1771923157356439\n",
      "E 0.17666354889305944\n",
      "E 0.17613197806076833\n",
      "E 0.17559755405385488\n",
      "E 0.17506023954800273\n",
      "E 0.17452000641744747\n",
      "E 0.17397683372363107\n",
      "E 0.1734307061983351\n",
      "E 0.17288161310187164\n",
      "E 0.17232954736527306\n",
      "E 0.17177450494719904\n",
      "E 0.17121648435295175\n",
      "E 0.17065548627571225\n",
      "E 0.17009151332980935\n",
      "E 0.1695245698531902\n",
      "E 0.16895466176185572\n",
      "E 0.1683817964432536\n",
      "E 0.16780598267881874\n",
      "E 0.16722723058827352\n",
      "E 0.1666455515901187\n",
      "E 0.16606095837412205\n",
      "E 0.1654734648826468\n",
      "E 0.16488308629843904\n",
      "E 0.16428983903707925\n",
      "E 0.16369374074274343\n",
      "E 0.1630948102862494\n",
      "E 0.16249306776461128\n",
      "E 0.1618885345015131\n",
      "E 0.16128123304825223\n",
      "E 0.16067118718480675\n",
      "E 0.1600584219207641\n",
      "E 0.15944296349590426\n",
      "E 0.15882483938027733\n",
      "E 0.158204078273652\n",
      "E 0.1575807101042325\n",
      "E 0.15695476602656724\n",
      "E 0.1563262784185834\n",
      "E 0.15569528087769668\n",
      "E 0.15506180821595292\n",
      "E 0.15442589645416857\n",
      "E 0.15378758281503982\n",
      "E 0.15314690571519893\n",
      "E 0.15250390475619843\n",
      "E 0.15185862071440884\n",
      "E 0.1512110955298183\n",
      "E 0.15056137229372726\n",
      "E 0.14990949523533215\n",
      "E 0.14925550970719617\n",
      "E 0.1485994621696083\n",
      "E 0.14794140017383234\n",
      "E 0.14728137234425312\n",
      "E 0.1466194283594262\n",
      "E 0.1459556189320427\n",
      "E 0.14528999578782079\n",
      "E 0.14462261164334053\n",
      "E 0.14395352018283666\n",
      "E 0.14328277603397213\n",
      "E 0.1426104347426105\n",
      "E 0.1419365527466141\n",
      "E 0.14126118734869078\n",
      "E 0.14058439668831993\n",
      "E 0.13990623971278512\n",
      "E 0.13922677614734683\n",
      "E 0.138546066464587\n",
      "E 0.13786417185296215\n",
      "E 0.1371811541846012\n",
      "E 0.13649707598238578\n",
      "E 0.13581200038635446\n",
      "E 0.13512599111947068\n",
      "E 0.13443911245279738\n",
      "E 0.13375142917012248\n",
      "E 0.1330630065320795\n",
      "E 0.13237391023980966\n",
      "E 0.131684206398211\n",
      "E 0.13099396147882295\n",
      "E 0.13030324228239387\n",
      "E 0.12961211590118024\n",
      "E 0.12892064968102587\n",
      "E 0.12822891118327115\n",
      "E 0.12753696814654028\n",
      "E 0.12684488844845745\n",
      "E 0.1261527400673403\n",
      "E 0.1254605910439187\n",
      "E 0.12476850944313074\n",
      "E 0.12407656331603954\n",
      "E 0.12338482066192362\n",
      "E 0.12269334939058385\n",
      "E 0.12200221728491607\n",
      "E 0.12131149196379339\n",
      "E 0.12062124084530323\n",
      "E 0.1199315311103829\n",
      "E 0.11924242966689554\n",
      "E 0.11855400311418791\n",
      "E 0.11786631770817062\n",
      "E 0.11717943932695758\n",
      "E 0.11649343343710382\n",
      "E 0.11580836506047615\n",
      "E 0.1151242987417906\n",
      "E 0.11444129851685038\n",
      "E 0.11375942788151332\n",
      "E 0.11307874976142003\n",
      "E 0.11239932648250811\n",
      "E 0.11172121974234028\n",
      "E 0.11104449058226822\n",
      "E 0.11036919936045615\n",
      "E 0.10969540572578285\n",
      "E 0.10902316859264201\n",
      "E 0.10835254611665661\n",
      "E 0.10768359567132287\n",
      "E 0.10701637382559641\n",
      "E 0.10635093632243253\n",
      "E 0.10568733805828945\n",
      "E 0.10502563306360287\n",
      "E 0.10436587448423777\n",
      "E 0.10370811456392155\n",
      "E 0.10305240462766185\n",
      "E 0.10239879506614953\n",
      "E 0.10174733532114703\n",
      "E 0.10109807387185908\n",
      "E 0.10045105822228356\n",
      "E 0.0998063348895365\n",
      "E 0.09916394939314532\n",
      "E 0.09852394624530292\n",
      "E 0.09788636894207303\n",
      "E 0.09725125995553674\n",
      "E 0.0966186607268699\n",
      "E 0.09598861166033675\n",
      "E 0.09536115211818907\n",
      "E 0.09473632041645297\n",
      "E 0.09411415382159145\n",
      "E 0.09349468854802295\n",
      "E 0.09287795975648143\n",
      "E 0.09226400155319818\n",
      "E 0.09165284698988778\n",
      "E 0.09104452806451813\n",
      "E 0.09043907572284575\n",
      "E 0.08983651986069441\n",
      "E 0.08923688932695788\n",
      "E 0.08864021192730392\n",
      "E 0.08804651442855922\n",
      "E 0.08745582256375228\n",
      "E 0.0868681610377927\n",
      "E 0.0862835535337636\n",
      "E 0.08570202271980598\n",
      "E 0.0851235902565705\n",
      "E 0.08454827680521541\n",
      "E 0.08397610203592688\n",
      "E 0.08340708463693931\n",
      "E 0.08284124232403293\n",
      "E 0.08227859185048578\n",
      "E 0.08171914901745737\n",
      "E 0.08116292868478282\n",
      "E 0.08060994478215308\n",
      "E 0.08006021032066202\n",
      "E 0.07951373740469624\n",
      "E 0.07897053724414786\n",
      "E 0.0784306201669283\n",
      "E 0.07789399563176251\n",
      "E 0.07736067224124372\n",
      "E 0.07683065775512743\n",
      "E 0.07630395910384638\n",
      "E 0.07578058240222661\n",
      "E 0.07526053296338543\n",
      "E 0.07474381531279341\n",
      "E 0.07423043320248243\n",
      "E 0.07372038962538219\n",
      "E 0.07321368682976791\n",
      "E 0.07271032633380307\n",
      "E 0.07221030894016119\n",
      "E 0.07171363475071017\n",
      "E 0.07122030318124564\n",
      "E 0.07073031297625731\n",
      "E 0.07024366222371488\n",
      "E 0.06976034836985999\n",
      "E 0.06928036823399096\n",
      "E 0.06880371802322745\n",
      "E 0.06833039334724335\n",
      "E 0.0678603892329563\n",
      "E 0.06739370013916217\n",
      "E 0.0669303199711044\n",
      "E 0.06647024209496794\n",
      "E 0.0660134593522876\n",
      "E 0.06555996407426261\n",
      "E 0.06510974809596708\n",
      "E 0.06466280277045001\n",
      "E 0.06421911898271492\n",
      "E 0.06377868716357295\n",
      "E 0.06334149730336254\n",
      "E 0.0629075389655279\n",
      "E 0.06247680130005142\n",
      "E 0.062049273056733294\n",
      "E 0.06162494259831338\n",
      "E 0.06120379791343048\n",
      "E 0.0607858266294136\n",
      "E 0.06037101602490199\n",
      "E 0.05995935304228887\n",
      "E 0.05955082429998642\n",
      "E 0.05914541610450774\n",
      "E 0.05874311446236354\n",
      "E 0.05834390509177043\n",
      "E 0.0579477734341689\n",
      "E 0.057554704665548545\n",
      "E 0.05716468370757899\n",
      "E 0.05677769523854495\n",
      "E 0.05639372370408426\n",
      "E 0.05601275332772764\n",
      "E 0.05563476812123928\n",
      "E 0.05525975189475881\n",
      "E 0.054887688266742445\n",
      "E 0.05451856067370501\n",
      "E 0.05415235237976184\n",
      "E 0.05378904648597086\n",
      "E 0.05342862593947622\n",
      "E 0.05307107354245278\n",
      "E 0.05271637196085316\n",
      "E 0.052364503732958166\n",
      "E 0.05201545127773105\n",
      "E 0.05166919690297794\n",
      "E 0.051325722813314545\n",
      "E 0.050985011117941535\n",
      "E 0.05064704383822958\n",
      "E 0.050311802915116444\n",
      "E 0.049979270216317206\n",
      "E 0.04964942754334952\n",
      "E 0.04932225663837677\n",
      "E 0.04899773919087017\n",
      "E 0.04867585684409218\n",
      "E 0.048356591201404155\n",
      "E 0.04803992383239944\n",
      "E 0.04772583627886471\n",
      "E 0.04741431006057236\n",
      "E 0.04710532668090559\n",
      "E 0.04679886763231916\n",
      "E 0.04649491440163794\n",
      "E 0.0461934484751965\n",
      "E 0.045894451343821384\n",
      "E 0.045597904507659076\n",
      "E 0.04530378948085286\n",
      "E 0.04501208779606975\n",
      "E 0.04472278100888192\n",
      "E 0.04443585070200359\n",
      "E 0.044151278489387547\n",
      "E 0.04386904602018251\n",
      "E 0.04358913498255529\n",
      "E 0.04331152710737956\n",
      "E 0.04303620417179421\n",
      "E 0.04276314800263397\n",
      "E 0.042492340479734714\n",
      "E 0.042223763539116464\n",
      "E 0.04195739917604616\n",
      "E 0.041693229447983364\n",
      "E 0.04143123647741106\n",
      "E 0.04117140245455425\n",
      "E 0.04091370963998919\n",
      "E 0.04065814036714486\n",
      "E 0.04040467704470061\n",
      "E 0.040153302158880806\n",
      "E 0.039903998275650464\n",
      "E 0.03965674804281298\n",
      "E 0.03941153419201332\n",
      "E 0.03916833954064841\n",
      "E 0.038927146993687295\n",
      "E 0.038687939545403525\n",
      "E 0.038450700281021356\n",
      "E 0.038215412378279155\n",
      "E 0.037982059108910816\n",
      "E 0.03775062384004865\n",
      "E 0.0375210900355489\n",
      "E 0.03729344125724249\n",
      "E 0.03706766116611294\n",
      "E 0.03684373352340342\n",
      "E 0.03662164219165518\n",
      "E 0.036401371135678685\n",
      "E 0.036182904423460234\n",
      "E 0.03596622622700529\n",
      "E 0.03575132082312064\n",
      "E 0.0355381725941372\n",
      "E 0.035326766028575034\n",
      "E 0.035117085721752705\n",
      "E 0.03490911637634234\n",
      "E 0.034702842802872134\n",
      "E 0.0344982499201781\n",
      "E 0.034295322755806594\n",
      "E 0.03409404644636894\n",
      "E 0.033894406237850165\n",
      "E 0.03369638748587291\n",
      "E 0.03349997565591826\n",
      "E 0.03330515632350478\n",
      "E 0.033111915174327176\n",
      "E 0.03292023800435611\n",
      "E 0.03273011071990033\n",
      "E 0.03254151933763231\n",
      "E 0.03235444998457934\n",
      "E 0.03216888889808019\n",
      "E 0.031984822425709665\n",
      "E 0.031802237025171494\n",
      "E 0.03162111926416134\n",
      "E 0.03144145582020017\n",
      "E 0.03126323348044011\n",
      "E 0.03108643914144324\n",
      "E 0.030911059808934188\n",
      "E 0.030737082597528484\n",
      "E 0.030564494730436265\n",
      "E 0.03039328353914376\n",
      "actual :\n",
      " [[1]\n",
      " [1]\n",
      " [0]] \n",
      "\n",
      "predicted :\n",
      " [[0.89253949]\n",
      " [0.82401525]\n",
      " [0.22059342]]\n"
     ]
    }
   ],
   "source": [
    "## Advanced Neural network in numpy 可自行設定層數\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Input array\n",
    "X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "y=np.array([[1],[1],[0]])\n",
    "\n",
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "#Sigmoid Function for backpropagation\n",
    "def derivatives_sigmoid (x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "#Variable initialization\n",
    "epoch=500 #Setting training iterations\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "hiddenlayer_neurons = 10 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons at output layer\n",
    "\n",
    "#weight and bias initialization\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons)) #均勻分布\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "for i in range(epoch):\n",
    "  #Forward Propogation\n",
    "  hidden_layer_input1=np.dot(X,wh)#做內積\n",
    "  hidden_layer_input=hidden_layer_input1 + bh\n",
    "  hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "  output_layer_input1=np.dot(hiddenlayer_activations,wout)\n",
    "  output_layer_input= output_layer_input1+ bout\n",
    "  output = sigmoid(output_layer_input)\n",
    "\n",
    "  #Backpropagation\n",
    "  E = y-output #Loss差異\n",
    "  back_output_layer = derivatives_sigmoid(output)\n",
    "  back_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "  d_output = E * back_output_layer\n",
    "  Error_at_hidden_layer = d_output.dot(wout.T) #轉置後才能乘back_hidden_layer\n",
    "  d_hiddenlayer = Error_at_hidden_layer * back_hidden_layer\n",
    "  wout += hiddenlayer_activations.T.dot(d_output) *lr #乘以學習率之後，再每次迴圈判斷Error\n",
    "  bout += np.sum(d_output, axis=0,keepdims=True) *lr #乘以學習率之後，再每次迴圈判斷Error\n",
    "  wh += X.T.dot(d_hiddenlayer) *lr \n",
    "  bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr #keepdims保持矩陣二維特性\n",
    "    \n",
    "  numpy_E = np.array(E) \n",
    "  Loss_E = np.sum((numpy_E)**2)/len(numpy_E)\n",
    "  print(\"E\",Loss_E)\n",
    "  \n",
    "    \n",
    "print('actual :\\n', y, '\\n')\n",
    "print('predicted :\\n', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual :\n",
      " [[1]\n",
      " [1]\n",
      " [0]] \n",
      "\n",
      "predicted :\n",
      " [[0.89253949]\n",
      " [0.82401525]\n",
      " [0.22059342]]\n"
     ]
    }
   ],
   "source": [
    "print('actual :\\n', y, '\\n')\n",
    "print('predicted :\\n', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先教Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual :\n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [0.]]) \n",
      "\n",
      "predicted :\n",
      " tensor([[0.9723],\n",
      "        [0.9356],\n",
      "        [0.0827]])\n"
     ]
    }
   ],
   "source": [
    "## neural network in pytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#Input array\n",
    "X = torch.Tensor([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "y = torch.Tensor([[1],[1],[0]])\n",
    "\n",
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "  return 1/(1 + torch.exp(-x))\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "#Variable initialization\n",
    "epoch=1000 #Setting training iterations\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "hiddenlayer_neurons = 10 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons at output layer\n",
    "\n",
    "#weight and bias initialization\n",
    "wh=torch.randn(inputlayer_neurons, hiddenlayer_neurons).type(torch.FloatTensor)\n",
    "bh=torch.randn(1, hiddenlayer_neurons).type(torch.FloatTensor) #1=size\n",
    "wout=torch.randn(hiddenlayer_neurons, output_neurons)\n",
    "bout=torch.randn(1, output_neurons)#1=size\n",
    "\n",
    "#wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons)) #均勻分布\n",
    "#bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "#wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "#bout=np.random.uniform(size=(1,output_neurons))\n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "  #Forward Propogation\n",
    "  hidden_layer_input1 = torch.mm(X, wh) # torch.mm = np.dot\n",
    "  hidden_layer_input = hidden_layer_input1 + bh\n",
    "  hidden_layer_activations = sigmoid(hidden_layer_input)\n",
    " \n",
    "  output_layer_input1 = torch.mm(hidden_layer_activations, wout)\n",
    "  output_layer_input = output_layer_input1 + bout\n",
    "  output = sigmoid(output_layer_input1)\n",
    "\n",
    "  #Backpropagation\n",
    "  E = y-output\n",
    "  back_output_layer = derivatives_sigmoid(output)\n",
    "  back_hidden_layer = derivatives_sigmoid(hidden_layer_activations)\n",
    "  d_output = E * back_output_layer\n",
    "  Error_at_hidden_layer = torch.mm(d_output, wout.t())\n",
    "  d_hiddenlayer = Error_at_hidden_layer * back_hidden_layer\n",
    "  wout += torch.mm(hidden_layer_activations.t(), d_output) *lr\n",
    "  bout += d_output.sum() *lr\n",
    "  wh += torch.mm(X.t(), d_hiddenlayer) *lr\n",
    "  bh += d_output.sum() *lr\n",
    "\n",
    "  ##Loss 先將數值轉numpy\n",
    "  #numpy_E = E.numpy() \n",
    "  #Loss_E = np.sum((numpy_E)**2)/len(numpy_E)\n",
    "  #print(\"E\",Loss_E)\n",
    "\n",
    "  #if i % 10 == 0: #取餘數，讓他變成10次顯示一次\n",
    "  #  print(\"Epoch: {} loss {}\".format(i, Loss_E))\n",
    " \n",
    "print('actual :\\n', y, '\\n')\n",
    "print('predicted :\\n', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#試試迭代次數變少\n",
    "#試試數減少\n",
    "#試試層數為1、epoch為10\n",
    "#加上MSE\n",
    "#整理迭代的內容，如何讓每十次顯示一個迭代\n",
    "#找一份opendata來試試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
